{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDiXl4ay5UM8"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "Welcome to the first assignment of CS163 Computer Vision course!\n",
        "\n",
        "First of all, please type your information below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rIeH92UsBmbO"
      },
      "outputs": [],
      "source": [
        "#@title Your Info { display-mode: \"form\" }\n",
        "\n",
        "Name = ' '  #@param {type:\"string\"}\n",
        "UID = ' '  #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y9NzXD89qUJ"
      },
      "source": [
        "## Goals\n",
        "In the first assignment, you will learn:\n",
        "* How to use Google Colab\n",
        "* How to implement a PyTorch data loader to load and preprocess the MiniPlaces dataset.\n",
        "* How to implement a linear/logistic/softmax regression classifier\n",
        "* How to build and train a fully-connected neural network from scratch and using built-in PyTorch modules.\n",
        "\n",
        "\n",
        "Free free to raise issues in the Piazza forum if you find any bugs or have any questions about the assignment.\n",
        "\n",
        "Please do not directly copy code from other sources and use any Code AI to finish the assignment.\n",
        "\n",
        "You could either complete the assignment using a GPU or CPU runtime. As the computation is not heavy for this assignment, there will not be much difference between GPU and CPU. However, if you would like to access to more GPU resources, we recommend purchasing the Colab Pro subscription for 10$/month.\n",
        "\n",
        "This assignment is due on **Sunday, Oct 20**. Good luck with your work!\n",
        "\n",
        "‚ùó**Once you have finished all the questions, it will still take approximately *60 minutes* to re-run the entire notebook in order to prepare the submission version. Make sure to allocate enough time for this task and start early.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U62cf5KB5Xbp"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this assignment, you will be working with the [MiniPlaces dataset](https://github.com/CSAILVision/miniplaces), a small-scale version of the [Places2 dataset](http://places2.csail.mit.edu/), which is a large-scale dataset of scene images (10+ million images) with a wide variety of real-world environments (400+ unique scene categories). The MiniPlaces dataset is a subset of the [Places2 dataset](http://places2.csail.mit.edu/) and contains 100,000 images for training, 10,000 images for validation, and 10,000 images for testing, each of which has been annotated with one of 100 different scene categories. These images are divided into three folders: train, val, and test.\n",
        "\n",
        "The MiniPlaces dataset is a useful resource for developing and testing image classification models, particularly those that are designed to recognize different types of environments and scenes.\n",
        "\n",
        "In this assignment, you will use PyTorch to load the MiniPlaces dataset and compare different image classification methods including linear/logistic/softmax regression classifier and a fully-connected neural network.\n",
        "\n",
        "Throughout the course, we will be using [PyTorch](https://pytorch.org/) for building and training different neural networks. It would be highly beneficial to go through the PyTorch [tutorials](https://pytorch.org/tutorials/) and get familiar with its APIs.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SIGjL2GBmOw"
      },
      "source": [
        "## Q0: Data Preparation(0 pts)\n",
        "\n",
        "Colab is a cloud-based service that allows users to write and run code in a Jupyter notebook-style enviroment. Colab provides users with temporary virtual machines to run their code on.\n",
        "\n",
        "For file storage, Colab provides users with access to a variety of storage options, including temporary storage space and Google Drive storage.\n",
        "\n",
        "1. Temporary storage space, also known as \"local runtime storage,\" is a space on the virtual machine (VM) that is allocated to you when you open a Colab notebook. This space is temporary in the sense that it is wiped clean whenever you close the notebook or disconnect from the VM. However, it is much faster than Google Drive storage, since it is located on the same machine as the VM. This makes it ideal for storing and accessing large datasets or intermediate data that is being used frequently in your notebook.\n",
        "\n",
        "2. Google Drive storage is a more permanent storage option that is accessed through your Google account. It is ***much much*** slower than temporary storage space, since it requires data to be transferred over the internet to and from the VM. However, it is useful for storing and sharing notebooks and data that you want to keep long-term, or for collaborating with other users.\n",
        "\n",
        "As we are going to be working with a lot of images, and to make things run smoothly, we will be using Colab's temporary storage space to store our data. This means that every time you open up this notebook, we will need to **re-download and process the dataset**. Don't worry though - this shouldn't take long, usually just a minute or less. Okay, let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdm6m71mWwKL"
      },
      "source": [
        "To begin, you will need to download the Miniplaces Dataset from its google drive file ID.\n",
        "\n",
        "Downloading this dataset typically takes less than 15 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2kAtfVUuwXq",
        "outputId": "35eb0fb3-d84b-4f75-90ff-2e408b92d54e"
      },
      "outputs": [],
      "source": [
        "# Downloading this file takes about a few seconds.\n",
        "# Download the tar.gz file from google drive using its file ID.\n",
        "!pip3 install --upgrade gdown --quiet\n",
        "!wget https://web.cs.ucla.edu/~smo3/data.tar.gz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSDHq8zXb24"
      },
      "source": [
        "In the following code, we setup the root directory for our project, extract the dataset to the root directory, and download the label files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ol_RX3yIBi3w",
        "outputId": "ca74f889-51d2-4394-87e5-7888c2700855"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Let's make our assignment directory\n",
        "CS163_path = './CS163_W24'\n",
        "os.makedirs(os.path.join(CS163_path, 'Assignment1', 'data'), exist_ok=True)\n",
        "# os.makedirs will create directories recursively, i.e., it will create the\n",
        "# directories and any missing parent directories if they do not exist.\n",
        "\n",
        "# Now, let's specify the assignment path we will be working with as the root.\n",
        "root_dir = os.path.join(CS163_path, 'Assignment1')\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def setup(file_link_dict={}):\n",
        "    # Open the tar.gz file\n",
        "    tar = tarfile.open(\"data.tar.gz\", \"r:gz\")\n",
        "    # Extract the file \"./Assignment1/data\" folder\n",
        "    total_size = sum(f.size for f in tar.getmembers())\n",
        "    with tqdm(total=total_size,\n",
        "              unit=\"B\",\n",
        "              unit_scale=True,\n",
        "              desc=\"Extracting tar.gz file\") as pbar:\n",
        "        for member in tar.getmembers():\n",
        "            tar.extract(member, os.path.join(root_dir, 'data'))\n",
        "            pbar.update(member.size)\n",
        "    # Close the tar.gz file\n",
        "    tar.close()\n",
        "\n",
        "    # Next, we download the train/val/test txt files:\n",
        "    for file_name, file_link in file_link_dict.items():\n",
        "        print(f'Downloding {file_name}.txt from {file_link}')\n",
        "        urllib.request.urlretrieve(file_link,\n",
        "                                   f'{root_dir}/data/{file_name}.txt')\n",
        "\n",
        "\n",
        "val_url = 'https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/val.txt'\n",
        "train_url = 'https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/train.txt'\n",
        "setup(file_link_dict={'train': train_url, 'val': val_url}, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuN658YrYTMa"
      },
      "source": [
        "Also, it is a good habit to setup the seed before running any experiment to ensure reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G8xuFZtAYfxS"
      },
      "outputs": [],
      "source": [
        "# Also, seed everything for reproducibility\n",
        "# code from https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964#file-seed_everything-py\n",
        "def seed_everything(seed: int):\n",
        "    import os\n",
        "    import random\n",
        "\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "seed_everything(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-1eXSIDuwXq"
      },
      "source": [
        "## Q1: PyTorch Dataset (20 pts)\n",
        "In this part, you will implement a PyTorch dataset to load and preprocess the MiniPlaces dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB1-KBjLuwXq"
      },
      "source": [
        "### Q1.1 Data Transform (7 pts)\n",
        "\n",
        "In this sub-question, you will be loading a sample image from the MiniPlaces dataset and applying a data transformation to it using the `torchvision.transforms.Compose` function.\n",
        "\n",
        "First, let's load a sample image from the `{root_dir}/data/images/train/a/abbey/00000001.jpg` file and print some properties about it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "UmBf9x22uwXr",
        "outputId": "c5abd6a5-cd10-49e2-8e35-db1bef0f6431"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image  # we call this library \"pillow\"\n",
        "\n",
        "# Load sample image\n",
        "image = Image.open(f'{root_dir}/data/images/train/a/abbey/00000001.jpg')\n",
        "print(f'Data type of my image: {type(image)}')\n",
        "print(f\"Shape of image: {np.array(image).shape}\")\n",
        "print(f'Channel mode:', image.mode)\n",
        "print(f\"Value range of image: {image.getextrema()}\")\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.title('PIL Image')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffo4TMXauwXq"
      },
      "source": [
        "You should be aware of the following data properties:\n",
        "* The loaded image is in the format of \"channel-last\", with the shape of (height, width, channels), instead of (channels, height, width).\n",
        "* The channel mode is RGB, meaning the channels are ordered as red, green, and blue.\n",
        "* The value range of the data is between 0 and 255, so the data type is most likely uint8.\n",
        "\n",
        "Now let's try loading the same image using **opencv**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "MVjNEh-U0KnW",
        "outputId": "32892e43-660a-4a5e-e34b-0b85960733cd"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "image_opencv = cv2.imread(f'{root_dir}/data/images/train/a/abbey/00000001.jpg')\n",
        "\n",
        "# Display image using matplotlib\n",
        "plt.imshow(image_opencv)\n",
        "plt.title('OpenCV Image')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS0VvxMt1ueT"
      },
      "source": [
        "It is important to keep in mind these subtle differences when loading images using **opencv** or **PIL**. Be mindful of these variations when working with image data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9g3XS98UuwXq"
      },
      "source": [
        "In addition to the sample image, you can also experiment with loading other images from the dataset for further exploration.\n",
        "\n",
        "---\n",
        "\n",
        "All images in the MiniPlaces dataset have a shape of (128, 128), and have a `uint8` (0~255) data type.\n",
        "\n",
        "Next, you should define a data transform function using [torchvision.transforms](https://pytorch.org/vision/0.9/transforms.html). Compose that resizes the image to 32x32, converts it to a tensor, and normalizes it using ImageNet statistics, i.e., `mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]`, be familiar with those numbers and remember to **always** normalize the image before feeding it to the networks. Finally, flatten the image to a $32\\times32\\times3$-D vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TtFMTnXaQIoP"
      },
      "outputs": [],
      "source": [
        "# define the ImageNet statistics\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "image_net_mean = torch.Tensor([0.485, 0.456, 0.406])\n",
        "image_net_std = torch.Tensor([0.229, 0.224, 0.225])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCkTTWILuwXr"
      },
      "source": [
        "First, Let's take a look at the `transforms.ToTensor()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCUo4jDTuwXr",
        "outputId": "a6c7a8c2-44ab-4691-e938-c4e78fb18ac1"
      },
      "outputs": [],
      "source": [
        "print('original image:',\n",
        "      f'  type: {type(image)}',\n",
        "      f'  shape: {np.array(image).shape}',\n",
        "      f'  values: max={np.max(image)}, min={np.min(image)}',\n",
        "      sep='\\n')\n",
        "\n",
        "_t_img = transforms.ToTensor()(image)\n",
        "print('after ToTensor transformation:',\n",
        "      f'  type: {type(_t_img)}',\n",
        "      f'  shape: {_t_img.shape}',\n",
        "      f'  values: max={_t_img.max()}, min={_t_img.min()}',\n",
        "      sep='\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DiK7OCgf9rb"
      },
      "source": [
        "Make sure you understand what `transforms.ToTensor()` does and complete the following data transform:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxNEyVi529D2"
      },
      "outputs": [],
      "source": [
        "# Define data transformation\n",
        "data_transform = transforms.Compose([\n",
        "    ################# Your Implementations #####################################\n",
        "    # TODO: Resize image to 32x32\n",
        "    ################# End of your Implementations ##############################\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    ################# Your Implementations #####################################\n",
        "    # TODO: Normalize image using ImageNet statistics\n",
        "    #     : Flatten the image with torch.flatten\n",
        "    ################# End of your Implementations ##############################\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FJ6WGm8uwXr"
      },
      "source": [
        "Let's see how `data_transform` works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2_seHCpuwXr",
        "outputId": "0d163f39-15bf-42b7-e32f-373b0d77217c"
      },
      "outputs": [],
      "source": [
        "transformed_img = data_transform(image)\n",
        "print('shape:', transformed_img.shape)\n",
        "print('values:', transformed_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDhNtYT6uwXr"
      },
      "source": [
        "You should get results similar to the following if you're using the same image (`f'{root_dir}/data/images/train/a/abbey/00000001.jpg`):\n",
        "\n",
        "```\n",
        "shape: torch.Size([3072])\n",
        "values: tensor([-1.6898, -1.2959, -0.9020,  ..., -0.6541, -0.7413, -0.8110])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WapAgWRuwXr"
      },
      "source": [
        "After defining the data transform function, you can use it to transform the sample image and visualize the result. Here is an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "SpdV_gkJuwXr",
        "outputId": "14f0d61e-0a0a-46c9-e1d3-fcf7f18d35f5"
      },
      "outputs": [],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image.resize((32, 32)))\n",
        "plt.title('original')\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(transformed_img.reshape(3, 32, 32).permute(1, 2, 0))\n",
        "plt.title('transformed')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrAY0RRSuwXr"
      },
      "source": [
        "You will find that the transformed image is significantly distorted in terms of color.\n",
        "\n",
        "This is caused by the normalization step. Therefore, we need to undo this step to recover the original image.\n",
        "\n",
        "To do this, let's write a `tensor_to_image` function that inverts the data transformation process. This function should take a tensor image as input and return a displayable image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "Rcx4m6yKuwXr",
        "outputId": "918c6555-9fca-4428-9db7-ce3e33b023f5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def tensor_to_image(image):\n",
        "    \"\"\"\n",
        "    Convert a tensor image back to a displayable image.\n",
        "\n",
        "    Args:\n",
        "        image (torch.Tensor): Tensor image to convert.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Displayable image.\n",
        "    \"\"\"\n",
        "    tensor_image = image.clone().detach()\n",
        "    # TODO: transform the tensor_image into a numpy_image that is displayable by plt.\n",
        "    ################# Your Implementations #####################################\n",
        "    # Hints:  Reshape image vector to (3, 32, 32)\n",
        "    #         Transpose tensor image to (32, 32, 3)\n",
        "    #         Undo normalization\n",
        "    #         Convert tensor image to numpy array\n",
        "    ################# End of your Implementations ##############################\n",
        "    return numpy_image\n",
        "\n",
        "\n",
        "# check your implemention\n",
        "plt.imshow(tensor_to_image(transformed_img))\n",
        "plt.title('transformed')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqEY3-ZTuwXr"
      },
      "source": [
        "### Q1.2 MiniPlaces Dataloader (7 pts)\n",
        "\n",
        "You will implement a pytorch data loader for the MiniPlaces dataset, following the tutorial [here](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/57a471142057f27da635118e88a99bf6/data_tutorial.ipynb\n",
        ").\n",
        "\n",
        "Complete the following cell and debug it if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hi77EpRNuwXr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class MiniPlaces(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir, split, transform=None, label_dict=None):\n",
        "        \"\"\"\n",
        "        Initialize the MiniPlaces dataset with the root directory for the images,\n",
        "        the split (train/val/test), an optional data transformation,\n",
        "        and an optional label dictionary.\n",
        "\n",
        "        Args:\n",
        "            root_dir (str): Root directory for the MiniPlaces images.\n",
        "            split (str): Split to use ('train', 'val', or 'test').\n",
        "            transform (callable, optional): Optional data transformation to apply to the images.\n",
        "            label_dict (dict, optional): Optional dictionary mapping integer labels to class names.\n",
        "        \"\"\"\n",
        "        assert split in ['train', 'val', 'test']\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.filenames = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Take a second to think why we need this line.\n",
        "        # Hints: training set / validation set / test set.\n",
        "        self.label_dict = label_dict if label_dict is not None else {}\n",
        "\n",
        "        # You should\n",
        "        #   1. Load the train/val/test text file based on the `split` argument and\n",
        "        #     store the image filenames and labels.\n",
        "        #   2. Extract the class names from the image filenames and store them in\n",
        "        #     self.label_dict.\n",
        "        #   3. Construct a label dict that maps integer labels to class names, if\n",
        "        #     the current split is \"train\"\n",
        "        ################# Your Implementations #####################################\n",
        "\n",
        "        ################# End of your Implementations ##############################\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the number of images in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of images in the dataset.\n",
        "        \"\"\"\n",
        "        dataset_len = 0\n",
        "        ################# Your Implementations #####################################\n",
        "        # Return the number of images in the dataset\n",
        "\n",
        "        ################# End of your Implementations ##############################\n",
        "        return dataset_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Return a single image and its corresponding label when given an index.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the image to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Tuple containing the image and its label.\n",
        "        \"\"\"\n",
        "        ################# Your Implementations #####################################\n",
        "        # Load and preprocess image using self.root_dir,\n",
        "        # self.filenames[idx], and self.transform (if specified)\n",
        "\n",
        "        ################# End of your Implementations ##############################\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELRKigneuwXr",
        "outputId": "e9dc0cd3-3006-4994-9e75-1376c3724db6"
      },
      "outputs": [],
      "source": [
        "seed_everything(0)\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "# Create MiniPlaces dataset object\n",
        "miniplaces_train = MiniPlaces(data_root,\n",
        "                              split='train',\n",
        "                              transform=data_transform)\n",
        "\n",
        "# Check our implementation\n",
        "print('len of trainining dataset:', len(miniplaces_train))\n",
        "print('label_dict:', miniplaces_train.label_dict)\n",
        "random_idxs = np.random.choice(len(miniplaces_train), 3)\n",
        "print('Example filenames:',\n",
        "      [miniplaces_train.filenames[i] for i in random_idxs])\n",
        "print('Example class IDs:', [miniplaces_train.labels[i] for i in random_idxs])\n",
        "print('Example class names:', [\n",
        "    miniplaces_train.label_dict[miniplaces_train.labels[i]]\n",
        "    for i in random_idxs\n",
        "])\n",
        "print()\n",
        "\n",
        "miniplaces_val = MiniPlaces(data_root, split='val', transform=data_transform)\n",
        "print('val label_dict:', miniplaces_val.label_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXlEK_-7uwXr"
      },
      "source": [
        "If you have successfully implemented the dataset, the above code should give you results like this:\n",
        "\n",
        "\n",
        "```\n",
        "len of trainining dataset: 100000\n",
        "label_dict: {0: 'abbey', 1: 'airport_terminal', 2: 'amphitheater', 3: 'amusement_park', 4: 'aquarium', 5: 'aqueduct', 6: 'art_gallery', 7: 'assembly_line', 8: 'auditorium', 9: 'badlands', 10: 'shop', 11: 'ballroom', 12: 'bamboo_forest', 13: 'banquet_hall', 14: 'bar', 15: 'baseball_field', 16: 'bathroom', 17: 'beauty_salon', 18: 'bedroom', 19: 'boat_deck', 20: 'bookstore', 21: 'botanical_garden', 22: 'bowling_alley', 23: 'boxing_ring', 24: 'bridge', 25: 'bus_interior', 26: 'butchers_shop', 27: 'campsite', 28: 'candy_store', 29: 'canyon', 30: 'cemetery', 31: 'chalet', 32: 'outdoor', 33: 'classroom', 34: 'clothing_store', 35: 'coast', 36: 'cockpit', 37: 'coffee_shop', 38: 'conference_room', 39: 'construction_site', 40: 'corn_field', 41: 'corridor', 42: 'courtyard', 43: 'dam', 44: 'sand', 45: 'dining_room', 46: 'driveway', 47: 'fire_station', 48: 'food_court', 49: 'fountain', 50: 'gas_station', 51: 'golf_course', 52: 'harbor', 53: 'highway', 54: 'hospital_room', 55: 'hot_spring', 56: 'outdoor', 57: 'iceberg', 58: 'kindergarden_classroom', 59: 'kitchen', 60: 'laundromat', 61: 'lighthouse', 62: 'living_room', 63: 'lobby', 64: 'locker_room', 65: 'outdoor', 66: 'martial_arts_gym', 67: 'outdoor', 68: 'mountain', 69: 'indoor', 70: 'office', 71: 'palace', 72: 'parking_lot', 73: 'phone_booth', 74: 'playground', 75: 'racecourse', 76: 'railroad_track', 77: 'rainforest', 78: 'restaurant', 79: 'river', 80: 'rock_arch', 81: 'runway', 82: 'shed', 83: 'shower', 84: 'ski_slope', 85: 'skyscraper', 86: 'slum', 87: 'football', 88: 'indoor', 89: 'staircase', 90: 'platform', 91: 'supermarket', 92: 'swamp', 93: 'outdoor', 94: 'east_asia', 95: 'outdoor', 96: 'trench', 97: 'valley', 98: 'volcano', 99: 'yard'}\n",
        "Example filenames: ['train/m/mountain/00000269.jpg', 'train/d/dam/00000568.jpg', 'train/c/courtyard/00000614.jpg']\n",
        "Example class IDs: [68, 43, 42]\n",
        "Example class names: ['mountain', 'dam', 'courtyard']\n",
        "\n",
        "val label_dict: {}\n",
        "```\n",
        "\n",
        "----\n",
        "Since validation and test annotation files do not have class information, we should pass the `label_dict` obtained from the training set to them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wot4ASXDuwXr",
        "outputId": "4f92346d-e62d-4e8f-b8d6-b0c5936dce4d"
      },
      "outputs": [],
      "source": [
        "miniplaces_val = MiniPlaces(data_root,\n",
        "                            split='val',\n",
        "                            transform=data_transform,\n",
        "                            label_dict=miniplaces_train.label_dict)\n",
        "\n",
        "print('val label_dict:', miniplaces_val.label_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPOEIdYQuwXr"
      },
      "source": [
        "Now you should see the same `label_dict` as the tranining set.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoT2T1BLuwXr"
      },
      "source": [
        "Next, let's visualize the image in each category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DLowdIZpuwXs",
        "outputId": "8823b854-018b-4690-8adc-09766c985aa7"
      },
      "outputs": [],
      "source": [
        "figure = plt.figure(figsize=(24, 24))\n",
        "cols, rows = 10, 10\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(miniplaces_train), size=(1, )).item()\n",
        "    img, label = miniplaces_train[sample_idx]\n",
        "    class_name = miniplaces_train.label_dict[label]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(class_name)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(tensor_to_image(img))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtcd2yyRg-nB"
      },
      "source": [
        "Finally, we define the pytorch `DataLoader` for loading images into batches of data, make sure you have followed the [tutorial](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/57a471142057f27da635118e88a99bf6/data_tutorial.ipynb\n",
        ") and understand what is the use of `DataLoader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "UvPSN7ryKv6e"
      },
      "outputs": [],
      "source": [
        "# Define the batch size and number of workers\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# Create DataLoader for training and validation sets\n",
        "train_loader = DataLoader(miniplaces_train,\n",
        "                          batch_size=batch_size,\n",
        "                          num_workers=num_workers,\n",
        "                          shuffle=True)\n",
        "val_loader = DataLoader(miniplaces_val,\n",
        "                        batch_size=batch_size,\n",
        "                        num_workers=num_workers,\n",
        "                        shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTNtSH_xtGMi"
      },
      "source": [
        "### Q1.3 Answer the following questions (6 pts):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQA8cyRT1vYq"
      },
      "source": [
        "a. In a few words, describe the differences between loading images using **PIL** and **opencv**. (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VevLBlLp4uiM"
      },
      "source": [
        "(Double click to edit)\n",
        "\n",
        "Your answer:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdKL8SA9uwXr"
      },
      "source": [
        "b. In 1-2 sentences, explain why do we need to normalize the images before training. (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AosTAoWuwXr"
      },
      "source": [
        "(Double click to edit)\n",
        "\n",
        "Your answer:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw3ZLeC3t55s"
      },
      "source": [
        "c. In 1-2 sentences, explain the use of PyTorch ```DataLoader``` and its different from the PyTorch ```Dataset```. (2 pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QF1YRD7upKk"
      },
      "source": [
        "(Double click to edit)\n",
        "\n",
        "Your answer:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obpZTVJ26hyE"
      },
      "source": [
        "## Q2: Regression (40 pts)\n",
        "\n",
        "For this question, you will be implementing and evaluating the performance of linear, logistic and softmax regression classifier on the TinyPlaces dataset.\n",
        "\n",
        "Recall some details:\n",
        " 1. Linear Regression models the relationship between an input and an output by fitting a linear function to the data. In this assignemnt, we will use the Mean Square Error (MSE) loss function to minimize the weights and biases of the linear function.\n",
        " 2. Logistic Regression is commonly used for binary classification tasks. It is an extension of linear regression that adds a logistic function to the linear output. We will use the Binary Cross Entropy Loss to optimize the logistic regression model.\n",
        " 3. Softmax regression is usually used for multiclass classification tasks. It is an extension of linear regression that adds a softmax function to the linear output and uses the Cross Entropy loss to optimize the model.\n",
        "\n",
        "\n",
        "\n",
        "The goal of this exercise is to go through a simple example of the data-driven image classification pipeline, and also to practice writing efficient, vectorized code in PyTorch.\n",
        "\n",
        "‚ùó\n",
        "You may not use any functions from `torch.nn` or `torch.nn.functional` in your implementation, except for the `torch.nn.functional.one_hot` used in Q2.2 and Q2.3.\n",
        "\n",
        "\n",
        "The goals of this exercise are to go through a simple example of the data-driven image classification pipeline, and also to practice writing efficient, vectorized code in [PyTorch](https://pytorch.org/).\n",
        "\n",
        "You may not use any functions from torch.nn or torch.nn.functional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfvRB8rdC1RQ"
      },
      "source": [
        " We will run the codes on GPU. Go to:\n",
        "Runtime->Change Runtime Type->Hardware Accelerator->GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE2Hg8wFC0Yh",
        "outputId": "1a752cf0-e339-4c33-dcce-7b16696942c1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Define the device to use for training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "    print('Good to go!')\n",
        "else:\n",
        "    print('Please set GPU via Edit -> Notebook Settings.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2sV9w1q3Pek",
        "outputId": "5565238c-fca5-42e2-c30d-7cbf84e517e9"
      },
      "outputs": [],
      "source": [
        "# This line of code gives you the information about GPU\n",
        "! nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcS_PwFfEwRu"
      },
      "source": [
        "### Q2.1 Linear Regression (10 pts)\n",
        "\n",
        "In this part of the question, you will implement linear regression for image classification on the TinyPlaces dataset. Your goal is to predict the category of an image based on its pixel values.\n",
        "\n",
        "To do this, you will need to complete the following steps:\n",
        "\n",
        "1. Initialize the weights and biases of the linear regression model and implement the linear function and the predict function in the LinearRegression class. If the prediction score is greater than 0.5, we consider the image to be of the outdoor category. Otherwise, we consider it to be of the indoor category.\n",
        "\n",
        "2. Use the fit function in the LinearRegression class to fit the linear regression model to the training data using gradient descent. You will need to set the learning rate and the number of epochs for the gradient descent algorithm.\n",
        "\n",
        "3. Evaluate the performance of the linear regression model on the training and validation datasets using the evaluate function in the LinearRegression class. This function should calculate the accuracy of the model on the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbXT1RrtTyDm"
      },
      "source": [
        "---\n",
        "\n",
        "We will implement linear regression step by step.\n",
        "\n",
        "Remember that for linear regression:\n",
        "\n",
        "$$y = WX+b$$\n",
        "\n",
        "In our case, $X$ is the flattened image vector with a dimenstion of $32\\times32\\times3=3072$. Thus, the weight $W$ should be compatible with the dim of $X$.\n",
        "\n",
        "We use the linear function to directly predict the label ranging from 0-99.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Below is the class you need to implement. Don't be scared. Each code block requires a very few lines of implementation.\n",
        "\n",
        "You do not need to finish all the class functions at once. Instead, you should finish them one by one and try to debug them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "eKbDCvNAEp4w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class LinearRegression(object):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        \"\"\"\n",
        "          Initialize the weights and biases using zeros\n",
        "\n",
        "          Parameters:\n",
        "              input_size (int): The input size (dimension of feature vectors)\n",
        "              output_size (int): The output size (dimension of output logits)\n",
        "\n",
        "          Returns:\n",
        "              None.\n",
        "        \"\"\"\n",
        "        # Initialize and set the device for the weights and biases using zeros \n",
        "        # Remember to set requires_grad=True\n",
        "        ################# Your Implementations ################################\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "    def linear(self, x):\n",
        "        # Implement the linear function using y = Wx + b\n",
        "        ################# Your Implementations ################################\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # To make the output shape compact.\n",
        "        return self.linear(x).squeeze()\n",
        "\n",
        "    def get_loss(self, preds, targets):\n",
        "        # Calculate the mean squared error between the predicted labels and the ground-truth labels\n",
        "        ################# Your Implementations ################################\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return loss\n",
        "\n",
        "    def fit(self, train_loader, val_loader, lr, epochs=5):\n",
        "        # Fit the linear regression model to the training data using gradient descent\n",
        "        # lr is the learning rate, epochs is the number of epochs\n",
        "\n",
        "        # To store validation accuracy\n",
        "        val_accs = []\n",
        "        train_losses = []\n",
        "        for epoch in range(epochs):\n",
        "            # Create a progress bar using tqdm\n",
        "            pbar = tqdm(total=len(train_loader),\n",
        "                        desc=f'Epoch {epoch + 1}/{epochs}',\n",
        "                        position=0,\n",
        "                        leave=True)\n",
        "            epoch_loss = 0\n",
        "            iter = 0\n",
        "            for images, labels in train_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # TODO: finish the training steps:\n",
        "                ################# Your Implementations ################################\n",
        "                # 1. Calculate the loss\n",
        "                # 2. Backpropagate the loss to compute the gradients\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # 3. Update the weights and biases using gradient descent ()\n",
        "                    # 4. Reset the gradients by calling grad.zero_()\n",
        "                    \n",
        "                ################# End of your Implementations ##########################\n",
        "                pbar.set_postfix(loss=loss.item(), lr=lr)\n",
        "                pbar.update()\n",
        "                epoch_loss += loss.item()\n",
        "                iter += 1\n",
        "            # Calculate the validation accuracy\n",
        "            val_acc = self.evaluate(val_loader)\n",
        "            val_accs.append(val_acc)\n",
        "            # Calculate the average training loss\n",
        "            epoch_loss = epoch_loss / iter\n",
        "            train_losses.append(epoch_loss)\n",
        "            # Update the progress bar with the validation accuracy and training loss\n",
        "            pbar.set_description(\n",
        "                f'val_acc: {val_acc:.3f}, train_loss: {epoch_loss:.3f}')\n",
        "        return val_accs, train_losses\n",
        "\n",
        "    def evaluate(self, data_loader):\n",
        "        # Evaluate the performance of the linear regression model on the dataset\n",
        "        count = 0\n",
        "        correct = 0\n",
        "        pbar = tqdm(range(len(data_loader)),\n",
        "                    desc='Evaluating:',\n",
        "                    position=0,\n",
        "                    leave=True)\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            preds = self.forward(images)\n",
        "            # Calculate the predicted labels\n",
        "            preds = preds.round().int()\n",
        "            correct += (preds == labels).sum().item()\n",
        "            count += labels.shape[0]\n",
        "            pbar.update()\n",
        "        return correct / count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG0lialr3jBu"
      },
      "source": [
        "Let's test the performance of zeros-initialized model.\n",
        "You should finish two functions: `__init__` and `linear` in the `LinearRegression` class to run the below cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0A6AHca2-TG",
        "outputId": "3e22f3a7-192e-46ee-cc76-ed4ec3053e20"
      },
      "outputs": [],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Try to debug using this cell\n",
        "linear_model = LinearRegression(3072, 1)\n",
        "train_acc = linear_model.evaluate(train_loader)\n",
        "val_acc = linear_model.evaluate(val_loader)\n",
        "print('train accuracy:', train_acc)\n",
        "print('val accuracy:', val_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTS_hvlc2pW0"
      },
      "source": [
        "The above code should give you an accuracy of exactly 0.01, since we're using zeros-initialization, therefore all the outputs will be 0.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Now, let's consider optimizing the linear model by gradient descent.\n",
        "\n",
        "Recall that to optimize the linear model using gradient descent, you need to follow these steps:\n",
        "1. Compute the losses between the predicted labels and ground-truth labels using the mean squared error (MSE) loss.\n",
        "\n",
        "2. Back-propagate the losses to get the gradients of the parameters. This is done automatically in PyTorch when you call the `loss.backward()` method.\n",
        "\n",
        "3. Get the parameters for the next iteration by subtracting the gradient multiplied by the learning rate. You can do this by updating the `self.W` and `self.b` parameters in the `fit()` function, note the `torch.no_grad()` context manager that we use to avoid gradient computation during parameter update and save memory.\n",
        "\n",
        "With the above hints, finish the ``get_loss`` and ``fit``and function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii17wRAF9gp1"
      },
      "source": [
        "Finally, we train the linear regression model for multiple epochs. You may encounter that the loss would go to infinity (nan) during traning, think about the reason for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRzp8zPu8gyD",
        "outputId": "52f3da43-f064-4b1e-d3ef-5e8bb91b8fc3"
      },
      "outputs": [],
      "source": [
        "# Run this\n",
        "linear_model_1 = LinearRegression(3072, 1)\n",
        "linear_model_2 = LinearRegression(3072, 1)\n",
        "# You can reduce the number of training epochs to debug. Running 1 epcoh takes about 1-2 mins.\n",
        "val_accs_1, losses_1 = linear_model_1.fit(train_loader, val_loader, lr=1e-4)\n",
        "# Try training with a higher learning rate and observe the results\n",
        "val_accs_2, losses_2 = linear_model_2.fit(train_loader, val_loader, lr=1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8u9PjZ8DZHh"
      },
      "source": [
        "#### Visualize the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "28G1Z5_zCIva",
        "outputId": "e0892eca-6733-4114-c23e-eac906cba4c9"
      },
      "outputs": [],
      "source": [
        "plt.plot(val_accs_1, label='LinearRegression, lr=1e-4')\n",
        "plt.plot(val_accs_2, label='LinearRegression, lr=1e-2')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Accuracy (%)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRoNS2YIERWQ"
      },
      "source": [
        "#### Visualize the training loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "pEopZbsiEVVh",
        "outputId": "abeebff7-fcf3-486c-bb33-e1c4a5698313"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses_1, label='LinearRegression, lr=1e-4')\n",
        "plt.plot(losses_2, label='LinearRegression, lr=1e-2')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DGOh6Fm_bMH"
      },
      "source": [
        "### Q2.2 Logistic Regression (10pts)\n",
        "\n",
        "Logistic regression is a classification algorithm that extends Linear Regression by applying a sigmoid function to the output of the linear model:\n",
        "\n",
        "$$\\frac{1}{1+e^{-x}}$$\n",
        "\n",
        "The common loss function used in logistic regression is the binary cross entropy (BCE) loss, which measures the difference between the predicted probability distribution and the true probability distribution of the target classes:\n",
        "\n",
        "$$\\mathrm{BCE_{Loss}}(p, y) = -{(y\\log(p) + (1-y)\\log(1-p))}$$\n",
        "\n",
        "Note that the sigmoid function could only predict the probablity for a single class. Hence, the output size of the logistic regression model should be 100 instead of 1.\n",
        "\n",
        "You need to implement the sigmoid function (5 pts) and the cross entropy loss (5pts)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "uyJW0sfdiMv4"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    # Implement the sigmoid function\n",
        "    # x is the predicted logits with shape (batch_size, output_size)\n",
        "    # Return the sigmoid output with shape (batch_size, output_size)\n",
        "    ################# Your Implementations ################################\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return output\n",
        "\n",
        "\n",
        "def binary_cross_entropy_loss(p, y):\n",
        "    # Calculate the binary cross entropy loss\n",
        "    # Return the binary cross entropy loss with shape (batch_size, output_size)\n",
        "    ################# Your Implementations ################################\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7GxPJ-yBSbE"
      },
      "source": [
        "Remember that Logistic Regression is an extension of Linear Regression, so we can implement it by inheriting from Linear Regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NrBfgSuBQ3o"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(LinearRegression):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LogisticRegression, self).__init__(input_size, output_size)\n",
        "\n",
        "    def get_loss(self, pred_logits, targets):\n",
        "        # Apply the sigmoid function to the predicted logits\n",
        "        # Convert the target to one-hot vectors\n",
        "        # Calculate the binary cross entropy loss\n",
        "        # Return the mean of the loss\n",
        "        # Hint: Convert the ground-truth labels to one-hot encoding using torch.nn.functional.one_hot()\n",
        "        ################# Your Implementations ################################\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, data_loader):\n",
        "        # Evaluate the performance of the logistic regression model on the dataset\n",
        "        count = 0\n",
        "        correct = 0\n",
        "        pbar = tqdm(range(len(data_loader)),\n",
        "                    desc='Evaluating:',\n",
        "                    position=0,\n",
        "                    leave=True)\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            preds = self.forward(images)\n",
        "            preds = preds.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            count += labels.shape[0]\n",
        "            pbar.update()\n",
        "        return correct / count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkXUCz6zBxOn",
        "outputId": "a6e9cbbb-0474-4c98-fb63-9b93c6051e59"
      },
      "outputs": [],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "logistic_model_1 = LogisticRegression(3072, 100)\n",
        "logistic_model_2 = LogisticRegression(3072, 100)\n",
        "# try different lrs\n",
        "val_accs_1, losses_1 = logistic_model_1.fit(train_loader, val_loader, lr=1e-4)\n",
        "val_accs_2, losses_2 = logistic_model_2.fit(train_loader, val_loader, lr=1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXeHt4Pvf9rd"
      },
      "source": [
        "Note that the initial loss should be around 0.69, which is -ln(0.5), that could serve as a very useful sanity check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X_S2lY41ihm"
      },
      "source": [
        "#### Visualize the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "mVXzaXOq1kpA",
        "outputId": "21d14ac9-e399-4668-f400-367266dddb7b"
      },
      "outputs": [],
      "source": [
        "plt.plot(val_accs_1, label='LogisticRegression, lr=1e-4')\n",
        "plt.plot(val_accs_2, label='LogisticRegression, lr=1e-2')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Accuracy (%)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byBB3C65Ki2u"
      },
      "source": [
        "#### Visualize the training loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "m5RGWc2zKmF0",
        "outputId": "78f71f2f-734f-460c-e1ee-3412440dc2f7"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses_1, label='LogisticRegression, lr=1e-4')\n",
        "plt.plot(losses_2, label='LogisticRegression, lr=1e-2')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz1F6flbHBMD"
      },
      "source": [
        "### Q2.3 Softmax Regression (5 pts)\n",
        "\n",
        "Softmax regression is a classification algorithm that extends logistic regression. It is used to predict the probability of a data point belonging to each of the K classes in a multi-class classification problem. In contrast to logistic regression, which only handles binary classification problems, softmax regression allows us to predict the probability of a data point belonging to any of K classes.\n",
        "\n",
        "The softmax function is a generalization of the sigmoid function, which is used in logistic regression. It maps the output of a linear model to a probability distribution over the K classes. The softmax function is defined as:\n",
        "\n",
        "$$\\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ \\mathrm{for}\\ i=1,2,\\dots,K$$\n",
        "\n",
        "where $z$ is the output vector of the linear model, and $K$ is the number of classes.\n",
        "\n",
        "The loss function for softmax regression is the cross entropy (CE) loss. It measures the difference between the predicted probability distribution and the true probability distribution of the target class. The CE loss is defined as:\n",
        "\n",
        "$$\\mathrm{CE_{LOSS}}(\\mathbf{p},y)=-{\\log(p_y)}$$\n",
        "\n",
        "where $p_y$ is the predicted probability of the data point belonging to the true class y.\n",
        "\n",
        "In the Softmax Regression class, you will need to implement:\n",
        "1. The softmax function that maps the output of the linear model to a probability distribution over the K classes.\n",
        "2. The CE loss function that measures the difference between the predicted probability distribution and the true probability distribution of the target class.\n",
        "3. The predict function that predicts the class of a data point based on the predicted probability distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "vUNVJMx2LoTN"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    # Implement the softmax function\n",
        "    # x is the predicted logits with shape (batch_size, output_size)\n",
        "    # Return the softmax output with shape (batch_size, output_size)\n",
        "    ################# Your Implementations ################################\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return output\n",
        "\n",
        "\n",
        "def cross_entropy_loss(p, y):\n",
        "    # Apply the softmax function to the predicted logits\n",
        "    # Calculate the cross entropy loss\n",
        "    # Return the cross entropy loss with shape (batch_size,)\n",
        "    ################# Your Implementations ################################\n",
        "\n",
        "    ################# End of your Implementations #################\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "BzYXxe6TGTjP"
      },
      "outputs": [],
      "source": [
        "class SoftmaxRegression(LogisticRegression):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(SoftmaxRegression, self).__init__(input_size, output_size)\n",
        "\n",
        "    def get_loss(self, pred_logits, targets):\n",
        "        # Convert the target to one-hot vectors\n",
        "        # Calculate the cross entropy loss\n",
        "        # Return the mean of the loss\n",
        "        # Hint: Convert the ground-truth labels to one-hot encoding using torch.nn.functional.one_hot()\n",
        "        ################# Your Implementations ################################\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBlVP4RIMQ4W",
        "outputId": "b19ee342-c46f-4f01-b028-baca4b81cc13"
      },
      "outputs": [],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "sfm_model = SoftmaxRegression(3072, 100)\n",
        "# TODO: try experimenting with different lrs and find the best one\n",
        "val_accs, losses = sfm_model.fit(train_loader, val_loader, lr=1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U2t5zv6f9re"
      },
      "source": [
        "Note that the initial loss should be around 4.6, which is -ln(0.01) or ln(#total number of classes), that could serve as a very useful sanity check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rndJ7OjaRHef"
      },
      "source": [
        "### Q2.4: Answer the following questions: (15 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwNwh17mf9re"
      },
      "source": [
        "a: Use your own words, explain when should we use ```torch.no_grad()```? (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkeoZxh-f9re"
      },
      "source": [
        "(Double click to edit)\n",
        "\n",
        "Your Answer:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrqib5Y9f9re"
      },
      "source": [
        "b: Use your own words, explain the difference between training iterations and training epochs. (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcvvuD4Ef9re"
      },
      "source": [
        "(Double click to edit)\n",
        "\n",
        "Your Answer:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP3ZRaT6f9re"
      },
      "source": [
        "c: Describe the results with different learning rates in Q2.1 and Q2.2 and discuss why you obtain those results? (4 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6__vxY1xf9re"
      },
      "source": [
        "(Double click to edit)\n",
        "\n",
        "Your Answer:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGT4K9JIf9re"
      },
      "source": [
        "d: Describe how did you determine the best learning rate in Q2.3? I can get a validation accuracy around 9.6%. How about you? (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwire862f9re"
      },
      "source": [
        "(Double click to edit)\n",
        "\n",
        "Your Answer:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4xg_y-xf9re"
      },
      "source": [
        "e: Use your own words, explain what's the difference between Linear Regression, Logistic Regression and Softmax Regression, and why their performance vary with the image classification problem?  (5 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMmcfB4Tf9re"
      },
      "source": [
        "(Double click to edit)\n",
        "\n",
        "Your Answer:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvRjtrd4f9re"
      },
      "source": [
        "## Q3: MLP Classifier (40 pts)\n",
        "\n",
        "We will implement an MLP from scratch step by step in the following sections.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3rzbanGf9re"
      },
      "source": [
        "\n",
        "### Q3.1 Using built-in modules (10 pts)\n",
        "In this question, you will implement a multi-layer perceptron (MLP) classifier using PyTorch's built-in `nn.Linear` and `F.relu` functions.\n",
        "\n",
        "Please refer to [this link](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/611efa3e10bb2b546f3a33742edc4ecc/modelsyt_tutorial.ipynb)\n",
        "for model writing.\n",
        "\n",
        "First, define the MLP class and implement the `__init__` method to define the layers of the MLP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "G6z44NO6f9re"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class FastMLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        \"\"\"\n",
        "        Initialize an MLP classifier.\n",
        "        You can use Pytorch's built-in nn.Linear function.\n",
        "        Input and output sizes of each layer:\n",
        "          1) fc1: input_size, hidden_size\n",
        "          2) fc2: hidden_size, hidden_size\n",
        "          3) fc3: hidden_size, num_classes\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Size of the input layer.\n",
        "            hidden_size (int): Size of the hidden layer.\n",
        "            num_classes (int): Number of classes in the output layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        self.fc3 = None\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Define the layers of the MLP\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the MLP classifier.\n",
        "        Using ReLU as the activation function after each layer, except for the output layer.\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, num_classes).\n",
        "        \"\"\"\n",
        "        # Hint: call ReLu just by F.relu, where F is imported in the top line of this cell.\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Implement the forward pass of the MLP classifier\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hBl0USdf9re"
      },
      "source": [
        "\n",
        "Then, define the training and evaluation functions to train and test the MLP classifier (You don't need to modify this part):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Yzh9uYBYf9re"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, val_loader, optimizer, criterion, device,\n",
        "          num_epochs):\n",
        "    \"\"\"\n",
        "    Train the MLP classifier on the training set and evaluate it on the validation set every epoch.\n",
        "\n",
        "    Args:\n",
        "        model (MLP): MLP classifier to train.\n",
        "        train_loader (torch.utils.data.DataLoader): Data loader for the training set.\n",
        "        val_loader (torch.utils.data.DataLoader): Data loader for the validation set.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer to use for training.\n",
        "        criterion (callable): Loss function to use for training.\n",
        "        device (torch.device): Device to use for training.\n",
        "        num_epochs (int): Number of epochs to train the model.\n",
        "    \"\"\"\n",
        "    # Place model on device\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        # Use tqdm to display a progress bar during training\n",
        "        with tqdm(total=len(train_loader),\n",
        "                  desc=f'Epoch {epoch + 1}/{num_epochs}',\n",
        "                  position=0,\n",
        "                  leave=True) as pbar:\n",
        "            for inputs, labels in train_loader:\n",
        "                # Move inputs and labels to device\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero out gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Compute the logits and loss\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                # Backpropagate the loss\n",
        "                loss.backward()\n",
        "\n",
        "                # Update the weights\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update the progress bar\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        avg_loss, accuracy = evaluate(model, val_loader, criterion, device)\n",
        "        print(\n",
        "            f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}'\n",
        "        )\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate the MLP classifier on the test set.\n",
        "\n",
        "    Args:\n",
        "        model (MLP): MLP classifier to evaluate.\n",
        "        test_loader (torch.utils.data.DataLoader): Data loader for the test set.\n",
        "        criterion (callable): Loss function to use for evaluation.\n",
        "        device (torch.device): Device to use for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        float: Average loss on the test set.\n",
        "        float: Accuracy on the test set.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0.0\n",
        "        num_correct = 0\n",
        "        num_samples = 0\n",
        "\n",
        "        for inputs, labels in test_loader:\n",
        "            # Move inputs and labels to device\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Compute the logits and loss\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Compute the accuracy\n",
        "            _, predictions = torch.max(logits, dim=1)\n",
        "            num_correct += (predictions == labels).sum().item()\n",
        "            num_samples += len(inputs)\n",
        "\n",
        "    # Compute the average loss and accuracy\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = num_correct / num_samples\n",
        "\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9L-_sukf9rf",
        "outputId": "70c8aa22-3f6c-4921-cbae-889a975b8794"
      },
      "outputs": [],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "model = FastMLP(input_size=3 * 32 * 32,\n",
        "                hidden_size=1024,\n",
        "                num_classes=len(miniplaces_train.label_dict))\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train the model\n",
        "train(model,\n",
        "      train_loader,\n",
        "      val_loader,\n",
        "      optimizer,\n",
        "      criterion,\n",
        "      device,\n",
        "      num_epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83OfdmlMf9rf"
      },
      "source": [
        "I can achieve a validation accuracy of 14.53% after training for five epochs. How about you?\n",
        "\n",
        "----\n",
        "Training this small MLP on our dataset can take approximately 10 minutes (5 epochs).\n",
        "\n",
        "Here is a trick for debugging: subsampling the dataset to obtain a small subset (torch.util.data.Subset). But, it's important to compare different methods under the same setting: all models should be trained using the same training set and evaluated on the same validation set.\n",
        "\n",
        "Once you are satisfied with the results, you can change back to the original dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCNuus0Hf9rf"
      },
      "source": [
        "### Q3.2 Building MLP from scratch (10 pts)\n",
        "\n",
        "In the next task, you will implement your own Multi-Layer Perceptron (MLP) without using built-in PyTorch modules.\n",
        "\n",
        "This task should be relatively straightforward, as you have already implemented most of the required components in Q2. You can simply copy and paste them here. However, be mindful of any changes in the input and output dimensions that may be required for this specific assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "awF9v_PCf9rf"
      },
      "outputs": [],
      "source": [
        "class MyLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, in_features, out_features):\n",
        "        \"\"\"\n",
        "        Initialize a linear layer.\n",
        "\n",
        "        Args:\n",
        "            in_features (int): Size of the input layer.\n",
        "            out_features (int): Size of the output layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: also try the zero initialization\n",
        "        self.weight = nn.Parameter(\n",
        "            torch.randn(in_features, out_features) * 0.01)\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "        ################# End of your Implementations ##############################\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the linear layer.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, out_features).\n",
        "        \"\"\"\n",
        "        ################# Your Implementations #####################################\n",
        "        # TODO: Implement the forward pass of the linear layer\n",
        "\n",
        "        ################# End of your Implementations ##############################\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-G0zz1Jf9rf"
      },
      "source": [
        "Recall the definition of ReLu in class. Implement the ReLU activation function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "VnoYSvGJf9rf"
      },
      "outputs": [],
      "source": [
        "def relu(input):\n",
        "    \"\"\"\n",
        "    Apply the ReLU activation function element-wise to the input tensor.\n",
        "\n",
        "    Args:\n",
        "        input (torch.Tensor): Input tensor with any shape.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output tensor with the same shape as the input tensor,\n",
        "        containing the element-wise ReLU of the input tensor.\n",
        "    \"\"\"\n",
        "    outputs = None\n",
        "    ################# Your Implementations #####################################\n",
        "\n",
        "    ################# End of your Implementations ##############################\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VexL0OAMf9rf"
      },
      "source": [
        "Implement your own MLP using `MyLinear` and your defined `relu`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "lA_T4bk5f9rf"
      },
      "outputs": [],
      "source": [
        "class MyMLP(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        \"\"\"\n",
        "        Initialize an MLP classifier.\n",
        "        You can use Pytorch's built-in nn.Linear function.\n",
        "        Input and output sizes of each layer:\n",
        "          1) fc1: input_size, hidden_size\n",
        "          2) fc2: hidden_size, hidden_size\n",
        "          3) fc3: hidden_size, num_classes\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Size of the input layer.\n",
        "            hidden_size (int): Size of the hidden layer.\n",
        "            num_classes (int): Number of classes in the output layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        self.fc3 = None\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Define the layers of the MLP using `MyLinear`\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the MLP classifier.\n",
        "        Using ReLU as the activation function after each layer, except for the output layer.\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, num_classes).\n",
        "        \"\"\"\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Implement the forward pass of the MLP classifier\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-1BWwrlf9rf"
      },
      "source": [
        "Next, implement your own cross entropy loss using functions defined in Q2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "_W4d9SIif9rf"
      },
      "outputs": [],
      "source": [
        "class MyCrossEntropy(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        \"\"\"\n",
        "        Calculate the cross entropy loss for the given input and target.\n",
        "\n",
        "        Args:\n",
        "            logits (torch.Tensor): Input tensor with shape (batch_size, output_size).\n",
        "            target (torch.Tensor): Target tensor with shape (batch_size,).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Cross entropy loss.\n",
        "        \"\"\"\n",
        "        # TODO: use the cross_entropy_loss function in Q2 to complete the loss\n",
        "        ################# Your Implementations ################################\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvigKZcyf9rf"
      },
      "source": [
        "Now, test our implementation. You should get similar results as using built-in modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EUuCpm9f9rf",
        "outputId": "0431b956-90fd-4d05-e57d-20b890eae58a"
      },
      "outputs": [],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "model = MyMLP(input_size=3 * 32 * 32,\n",
        "              hidden_size=1024,\n",
        "              num_classes=len(miniplaces_train.label_dict))\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Define a cross entropy loss function using the MyCrossEntropy class\n",
        "criterion = MyCrossEntropy()\n",
        "\n",
        "# train_loader and val_loader have been declared before.\n",
        "# Train the model\n",
        "train(model,\n",
        "      train_loader,\n",
        "      val_loader,\n",
        "      optimizer,\n",
        "      criterion,\n",
        "      device,\n",
        "      num_epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJsNn3Ahf9rf"
      },
      "source": [
        "If your implementation is correct, you should get a similar validation accuracy (I got 14.06%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "621j3lyHf9rf"
      },
      "source": [
        "### Q3.3 SGD Optimizer (5 pts)\n",
        "\n",
        "In the next task, you will implement the Stochastic Gradient Descent (SGD) optimizer from scratch.\n",
        "\n",
        "Please note that for the purpose of this assignment, we will not be using techniques such as weight decay, damping or Nesterov. There are different variations of SGD implementations, and we have chosen this implementation for its simplicity. The pseudocode provided is a simplified version of the official implementation in PyTorch.\n",
        "\n",
        "Please follow the pseudocode provided to implement the optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncVHJTXPf9rf"
      },
      "source": [
        "\\begin{aligned}\n",
        "            &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
        "            &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n",
        "                \\text{ (objective)}, \\: \\\\\n",
        "            &\\hspace{13mm} \\:\\mu \\text{ (momentum)},\n",
        "            \\\\[-1.ex]\n",
        "            &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
        "            &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
        "            &\\hspace{5mm}\\textbf{g}_t           \\leftarrow   \\nabla_{\\theta} f_t (\\boldsymbol{\\theta}_{t-1})           \\\\\n",
        "            &\\hspace{5mm}\\textbf{if} \\: t > 1                                                   \\\\\n",
        "            &\\hspace{10mm} \\textbf{g}_t \\leftarrow \\mu \\textbf{g}_{t-1} + \\textbf{g}_t           \\\\\n",
        "            &\\hspace{5mm}\\textbf{else}                                                          \\\\\n",
        "            &\\hspace{10mm} \\textbf{g}_t \\leftarrow \\textbf{g}_t                                           \\\\\n",
        "            &\\hspace{5mm}\\boldsymbol{\\theta}_t \\leftarrow \\boldsymbol{\\theta}_{t-1} - \\gamma \\textbf{g}_t                   \\\\[-1.ex]\n",
        "            &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
        "            &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
        "            &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
        "       \\end{aligned}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ySS-1ctnf9rf"
      },
      "outputs": [],
      "source": [
        "class MySGD():\n",
        "\n",
        "    def __init__(self, params, lr=0.01, momentum=0.9):\n",
        "        \"\"\"\n",
        "        Initialize the MySGD optimizer.\n",
        "        Args:\n",
        "          params (iterable): An iterable of parameters to optimize.\n",
        "          lr (float, optional): learning rate. (default: 0.01)\n",
        "          momentum (float, optional): momentum factor. (default: 0.9)\n",
        "        \"\"\"\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.grad_buffer = {}  # store gradients in the previous step\n",
        "\n",
        "    def zero_grad(self):\n",
        "        # Set the gradients of all parameters to zero.\n",
        "        for param in self.params:\n",
        "            if param.grad is not None:\n",
        "                param.grad.zero_()\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Perform one optimization step on the parameters.\n",
        "        \"\"\"\n",
        "        for i, param in enumerate(self.params):\n",
        "            if param.grad is None:\n",
        "                continue\n",
        "            # use i as a key to retrieve velocity from self.grad_buffer.\n",
        "            # use param.data to update its value.\n",
        "            ################# Your Implementations ################################\n",
        "\n",
        "            ################# End of your Implementations #################\n",
        "            # You don't need to return anything"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5GN72xGf9rf"
      },
      "source": [
        "Now, test your own implementation. You should get similar results as Q3.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "lc-o2Ywzf9rf",
        "outputId": "abd05ec2-92ba-4761-e6ff-e2c526f26e5f"
      },
      "outputs": [],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "model = MyMLP(input_size=3 * 32 * 32,\n",
        "              hidden_size=1024,\n",
        "              num_classes=len(miniplaces_train.label_dict))\n",
        "optimizer = MySGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Define a cross entropy loss function using the MyCrossEntropy class\n",
        "criterion = MyCrossEntropy()\n",
        "\n",
        "# train_loader and val_loader have been declared before.\n",
        "# Train the model\n",
        "train(model,\n",
        "      train_loader,\n",
        "      val_loader,\n",
        "      optimizer,\n",
        "      criterion,\n",
        "      device,\n",
        "      num_epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyU4HAlGf9rg"
      },
      "source": [
        "### Q3.4 Adam Optimizer (5 pts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdZG7NXnf9rg"
      },
      "source": [
        "----\n",
        "Next, you will implement the Adam optimizer from scratch.\n",
        "\n",
        "Note that here we don't use weight decay or amsgrad here. There are different variants of Adam implementations. We choose this implementation for simplicity. The pseudocode here is a simplified version from the official implementation in Pytorch.\n",
        "\n",
        "Follow the pseudocode provided to implement your own optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLJ2_FhZf9rg"
      },
      "source": [
        "\\begin{aligned}\n",
        "            &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
        "            &\\textbf{input}      : \\gamma \\text{ (lr)}, \\beta_1, \\beta_2\n",
        "                \\text{ (betas)},\\theta_0 \\text{ (params)},f(\\theta) \\text{ (objective)}          \\\\\n",
        "            &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\n",
        "                v_0\\leftarrow 0 \\text{ (second moment)}\\\\[-1.ex]\n",
        "            &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
        "            &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
        "            &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n",
        "            &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n",
        "            &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n",
        "            &\\hspace{5mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n",
        "            &\\hspace{5mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                   \\\\\n",
        "            &\\hspace{5mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/\n",
        "                \\big(\\sqrt{\\hat{v_t}} + \\epsilon \\big)                                       \\\\\n",
        "            &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
        "            &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
        "            &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
        "       \\end{aligned}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "uu97E5Luf9rg"
      },
      "outputs": [],
      "source": [
        "class MyAdam():\n",
        "\n",
        "    def __init__(self, params, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        \"\"\"\n",
        "        Initialize the MyAdam optimizer.\n",
        "        Args:\n",
        "          params (iterable): An iterable of parameters to optimize.\n",
        "          lr (float, optional): learning rate. (default: 0.01)\n",
        "          beta1 (float, optional): first moment factor. (default: 0.9)\n",
        "          beta2 (float, optional): second moment factor. (default: 0.999)\n",
        "          eps (float, optional): term to avoid division by zero. (default: 1e-8)\n",
        "        \"\"\"\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.first_moments = {}  # store first moments in the previous step\n",
        "        self.second_moments = {}  # store second moments in the previous step\n",
        "        # Iteration counter\n",
        "        self.t = 0\n",
        "\n",
        "    def zero_grad(self):\n",
        "        # Set the gradients of all parameters to zero.\n",
        "        for param in self.params:\n",
        "            if param.grad is not None:\n",
        "                param.grad.zero_()\n",
        "\n",
        "    def step(self):\n",
        "        self.t += 1\n",
        "        for i, param in enumerate(self.params):\n",
        "            if param.grad is None:\n",
        "                continue\n",
        "            # use i as a key to retrieve velocity from self.first_moments and self.second_moments.\n",
        "            # use param.data to update its value.\n",
        "            ################# Your Implementations ################################\n",
        "\n",
        "            ################# End of your Implementations #################\n",
        "            # You don't need to return anything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UueP9vG3f9rg",
        "outputId": "3fdf74c4-dea7-488d-ac50-5febadcc5e92"
      },
      "outputs": [],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "model = MyMLP(input_size=3 * 32 * 32,\n",
        "              hidden_size=1024,\n",
        "              num_classes=len(miniplaces_train.label_dict))\n",
        "\n",
        "optimizer = None\n",
        "######## Your own implementation ################\n",
        "optimizer = MyAdam(model.parameters(), lr=0.001)\n",
        "#################################################\n",
        "\n",
        "# Define a cross entropy loss function using the MyCrossEntropy class\n",
        "criterion = None\n",
        "########## Your own implementation ########\n",
        "criterion = MyCrossEntropy()\n",
        "\n",
        "###########################################\n",
        "\n",
        "# train_loader and val_loader have been declared before.\n",
        "# Train the model\n",
        "train(model,\n",
        "      train_loader,\n",
        "      val_loader,\n",
        "      optimizer,\n",
        "      criterion,\n",
        "      device,\n",
        "      num_epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHAcNK11f9rg"
      },
      "source": [
        "I got an accuracy of 9.49% using my own implementation. How about you?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zwAorQZf9rg"
      },
      "source": [
        "### Q3.5 Answer the following questions (10pts):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aunbxsAl3p4n"
      },
      "source": [
        "a. Check the `train` function in Q3.1, in a few words, explain the role of a PyTorch optimizer. (2 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-vMkj3E3_Wu"
      },
      "source": [
        "(Double click to edit)\n",
        "\n",
        "Your answer:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzMKugBwf9rg"
      },
      "source": [
        "b. In Q3.2, try using zero initialization for the weight and bias. What result did you observe? Can you explain why you observe such results? (4 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhpzna26ypW8"
      },
      "source": [
        "(Double click to edit)\n",
        "\n",
        "Your answer:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t5BV1k9yqBt"
      },
      "source": [
        "c. Compare the performance between using MLPs and the softmax regression in Q2. In 1-2 sentences, explain why using MLPs leads to a better performance. (4 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctN__LgPy548"
      },
      "source": [
        "(Double click to edit)\n",
        "\n",
        "Your answer:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pSurRsp0Ush"
      },
      "source": [
        "Congratulations! You have done a great job in completing Assignment 1. In the Assigment 2, we will explore using convolutional neural networks for image classificiation which would lead to even better performance!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
